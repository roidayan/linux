From: Alaa Hleihel <alaa@mellanox.com>
Subject: [PATCH] BACKPORT: ib-core

Change-Id: Ie209820349292d72897ef4a621c850ee575379c8
Signed-off-by: Alaa Hleihel <alaa@mellanox.com>
---
 drivers/infiniband/core/addr.c             |  107 ++++++++++++++++++++++++++++
 drivers/infiniband/core/cm.c               |   23 ++++++
 drivers/infiniband/core/cma.c              |   64 +++++++++++++++++
 drivers/infiniband/core/fmr_pool.c         |    7 ++
 drivers/infiniband/core/iwcm.c             |   16 ++++
 drivers/infiniband/core/iwpm_util.c        |   26 ++++++-
 drivers/infiniband/core/mad.c              |    3 +
 drivers/infiniband/core/netlink.c          |   23 ++++++
 drivers/infiniband/core/roce_gid_cache.c   |    4 +
 drivers/infiniband/core/roce_gid_mgmt.c    |   50 +++++++++++++-
 drivers/infiniband/core/sa_query.c         |   19 +++++
 drivers/infiniband/core/ucm.c              |   38 ++++++++++
 drivers/infiniband/core/ucma.c             |   76 ++++++++++++++++++++
 drivers/infiniband/core/umem.c             |   44 +++++++++++
 drivers/infiniband/core/user_mad.c         |   19 +++++
 drivers/infiniband/core/uverbs_cmd.c       |   51 +++++++++++++
 drivers/infiniband/core/uverbs_main.c      |   40 ++++++++++
 drivers/infiniband/hw/mlx4/main.c          |   24 ++++++-
 drivers/infiniband/hw/mlx4/mr.c            |   18 +++++
 drivers/infiniband/hw/qib/qib_user_pages.c |    9 ++-
 include/rdma/ib_pack.h                     |    4 +
 include/rdma/ib_umem_odp.h                 |    4 +
 include/rdma/ib_verbs.h                    |    5 ++
 23 files changed, 668 insertions(+), 6 deletions(-)

diff --git a/drivers/infiniband/core/addr.c b/drivers/infiniband/core/addr.c
index xxxxxxx..xxxxxxx xxxxxx
--- a/drivers/infiniband/core/addr.c
+++ b/drivers/infiniband/core/addr.c
@@ -199,28 +199,45 @@ static void queue_req(struct addr_req *req)
 	mutex_unlock(&lock);
 }
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,39)
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,1,0)
 static int dst_fetch_ha(struct dst_entry *dst, struct rdma_dev_addr *dev_addr, void *daddr)
+#else
+static int dst_fetch_ha(struct dst_entry *dst, struct rdma_dev_addr *addr)
+#endif
 {
 	struct neighbour *n;
 	int ret;
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,1,0)
 	n = dst_neigh_lookup(dst, daddr);
+#endif
 
 	rcu_read_lock();
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3,3,0)
+	n = dst_get_neighbour(dst);
+#endif
 	if (!n || !(n->nud_state & NUD_VALID)) {
 		if (n)
 			neigh_event_send(n, NULL);
 		ret = -ENODATA;
 	} else {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,1,0)
 		ret = rdma_copy_addr(dev_addr, dst->dev, n->ha);
+#else
+		ret = rdma_copy_addr(addr, dst->dev, n->ha);
+#endif
 	}
 	rcu_read_unlock();
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,1,0)
 	if (n)
 		neigh_release(n);
+#endif
 
 	return ret;
 }
+#endif
 
 static int addr4_resolve(struct sockaddr_in *src_in,
 			 struct sockaddr_in *dst_in,
@@ -229,9 +246,15 @@ static int addr4_resolve(struct sockaddr_in *src_in,
 	__be32 src_ip = src_in->sin_addr.s_addr;
 	__be32 dst_ip = dst_in->sin_addr.s_addr;
 	struct rtable *rt;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,39)
 	struct flowi4 fl4;
+#else
+	struct flowi fl;
+	struct neighbour *neigh;
+#endif
 	int ret;
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,39)
 	memset(&fl4, 0, sizeof(fl4));
 	fl4.daddr = dst_ip;
 	fl4.saddr = src_ip;
@@ -241,10 +264,25 @@ static int addr4_resolve(struct sockaddr_in *src_in,
 		ret = PTR_ERR(rt);
 		goto out;
 	}
+#else
+	memset(&fl, 0, sizeof(fl));
+	fl.nl_u.ip4_u.daddr = dst_ip;
+	fl.nl_u.ip4_u.saddr = src_ip;
+	fl.oif = addr->bound_dev_if;
+	ret = ip_route_output_key(&init_net, &rt, &fl);
+	if (ret)
+		goto out;
+#endif
 	src_in->sin_family = AF_INET;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,39)
 	src_in->sin_addr.s_addr = fl4.saddr;
 
 	if (rt->dst.dev->flags & IFF_LOOPBACK) {
+#else
+	src_in->sin_addr.s_addr = rt->rt_src;
+
+	if (rt->idev->dev->flags & IFF_LOOPBACK) {
+#endif
 		ret = rdma_translate_ip((struct sockaddr *)dst_in, addr, NULL);
 		if (!ret)
 			memcpy(addr->dst_dev_addr, addr->src_dev_addr, MAX_ADDR_LEN);
@@ -252,15 +290,42 @@ static int addr4_resolve(struct sockaddr_in *src_in,
 	}
 
 	/* If the device does ARP internally, return 'done' */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,39)
 	if (rt->dst.dev->flags & IFF_NOARP) {
 		ret = rdma_copy_addr(addr, rt->dst.dev, NULL);
 		goto put;
 	}
 
+#ifdef HAVE_RT_USES_GATEWAY
 	if (rt->rt_uses_gateway)
 		addr->network = RDMA_NETWORK_IPV4;
+#endif
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,1,0)
 	ret = dst_fetch_ha(&rt->dst, addr, &fl4.daddr);
+#else
+	ret = dst_fetch_ha(&rt->dst, addr);
+#endif
+#else
+	if (rt->idev->dev->flags & IFF_NOARP) {
+		ret = rdma_copy_addr(addr, rt->idev->dev, NULL);
+		goto put;
+	}
+
+	neigh = neigh_lookup(&arp_tbl, &rt->rt_gateway, rt->idev->dev);
+	if (!neigh || !(neigh->nud_state & NUD_VALID)) {
+		neigh_event_send(rt->u.dst.neighbour, NULL);
+		ret = -ENODATA;
+		if (neigh)
+			goto release;
+		goto put;
+	}
+
+	ret = rdma_copy_addr(addr, neigh->dev, neigh->ha);
+release:
+	neigh_release(neigh);
+#endif
+
 put:
 	ip_rt_put(rt);
 out:
@@ -272,11 +337,17 @@ static int addr6_resolve(struct sockaddr_in6 *src_in,
 			 struct sockaddr_in6 *dst_in,
 			 struct rdma_dev_addr *addr)
 {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,39)
 	struct flowi6 fl6;
+#else
+	struct flowi fl;
+	struct neighbour *neigh;
+#endif
 	struct dst_entry *dst;
 	struct rt6_info *rt;
 	int ret;
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,39)
 	memset(&fl6, 0, sizeof fl6);
 	fl6.daddr = dst_in->sin6_addr;
 	fl6.saddr = src_in->sin6_addr;
@@ -296,6 +367,26 @@ static int addr6_resolve(struct sockaddr_in6 *src_in,
 		src_in->sin6_family = AF_INET6;
 		src_in->sin6_addr = fl6.saddr;
 	}
+#else
+	memset(&fl, 0, sizeof fl);
+	ipv6_addr_copy(&fl.fl6_dst, &dst_in->sin6_addr);
+	ipv6_addr_copy(&fl.fl6_src, &src_in->sin6_addr);
+	fl.oif = addr->bound_dev_if;
+
+	dst = ip6_route_output(&init_net, NULL, &fl);
+	if ((ret = dst->error))
+		goto put;
+
+	if (ipv6_addr_any(&fl.fl6_src)) {
+		ret = ipv6_dev_get_saddr(&init_net, ip6_dst_idev(dst)->dev,
+					 &fl.fl6_dst, 0, &fl.fl6_src);
+		if (ret)
+			goto put;
+
+		src_in->sin6_family = AF_INET6;
+		ipv6_addr_copy(&src_in->sin6_addr, &fl.fl6_src);
+	}
+#endif
 
 	if (dst->dev->flags & IFF_LOOPBACK) {
 		ret = rdma_translate_ip((struct sockaddr *)dst_in, addr, NULL);
@@ -313,7 +404,23 @@ static int addr6_resolve(struct sockaddr_in6 *src_in,
 	if (rt->rt6i_flags & RTF_GATEWAY)
 		addr->network = RDMA_NETWORK_IPV6;
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,39)
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,1,0)
 	ret = dst_fetch_ha(dst, addr, &fl6.daddr);
+#else
+	ret = dst_fetch_ha(dst, addr);
+#endif
+#else
+	neigh = dst->neighbour;
+	if (!neigh || !(neigh->nud_state & NUD_VALID)) {
+		neigh_event_send(dst->neighbour, NULL);
+		ret = -ENODATA;
+		goto put;
+	}
+
+	ret = rdma_copy_addr(addr, dst->dev, neigh->ha);
+#endif
+
 put:
 	dst_release(dst);
 	return ret;
diff --git a/drivers/infiniband/core/cm.c b/drivers/infiniband/core/cm.c
index xxxxxxx..xxxxxxx xxxxxx
--- a/drivers/infiniband/core/cm.c
+++ b/drivers/infiniband/core/cm.c
@@ -384,6 +384,7 @@ static int cm_init_av_by_path(struct ib_sa_path_rec *path, struct cm_av *av)
 
 static int cm_alloc_id(struct cm_id_private *cm_id_priv)
 {
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(3, 9, 0))
 	unsigned long flags;
 	int id;
 
@@ -397,6 +398,24 @@ static int cm_alloc_id(struct cm_id_private *cm_id_priv)
 
 	cm_id_priv->id.local_id = (__force __be32)id ^ cm.random_id_operand;
 	return id < 0 ? id : 0;
+#else
+	unsigned long flags;
+	int ret, id;
+	static int next_id;
+
+	do {
+		spin_lock_irqsave(&cm.lock, flags);
+		ret = idr_get_new_above(&cm.local_id_table, cm_id_priv,
+					next_id, &id);
+		if (!ret)
+			next_id = ((unsigned) id + 1) & MAX_IDR_MASK;
+
+		spin_unlock_irqrestore(&cm.lock, flags);
+	} while( (ret == -EAGAIN) && idr_pre_get(&cm.local_id_table, GFP_KERNEL) );
+
+	cm_id_priv->id.local_id = (__force __be32)id ^ cm.random_id_operand;
+	return ret;
+#endif
 }
 
 static void cm_free_id(__be32 local_id)
@@ -3673,7 +3692,11 @@ static struct kobj_type cm_port_obj_type = {
 	.release = cm_release_port_obj
 };
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,3,0)
 static char *cm_devnode(struct device *dev, umode_t *mode)
+#else
+static char *cm_devnode(struct device *dev, mode_t *mode)
+#endif
 {
 	if (mode)
 		*mode = 0666;
diff --git a/drivers/infiniband/core/cma.c b/drivers/infiniband/core/cma.c
index xxxxxxx..xxxxxxx xxxxxx
--- a/drivers/infiniband/core/cma.c
+++ b/drivers/infiniband/core/cma.c
@@ -2115,8 +2115,10 @@ static int cma_resolve_iw_route(struct rdma_id_private *id_priv, int timeout_ms)
 	return 0;
 }
 
+#if defined(HAVE_VLAN_DEV_GET_EGRESS_QOS_MASK) && defined(HAVE_NETDEV_GET_PRIO_TC_MAP)
 static int iboe_tos_to_sl(struct net_device *ndev, int tos)
 {
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(3, 0, 0))
 	int prio;
 	struct net_device *dev;
 
@@ -2132,8 +2134,10 @@ static int iboe_tos_to_sl(struct net_device *ndev, int tos)
 		return (vlan_dev_get_egress_qos_mask(ndev, prio) &
 			VLAN_PRIO_MASK) >> VLAN_PRIO_SHIFT;
 #endif
+#endif
 	return 0;
 }
+#endif
 
 static int cma_resolve_iboe_route(struct rdma_id_private *id_priv)
 {
@@ -2186,7 +2190,16 @@ static int cma_resolve_iboe_route(struct rdma_id_private *id_priv)
 	route->path_rec->reversible = 1;
 	route->path_rec->pkey = cpu_to_be16(0xffff);
 	route->path_rec->mtu_selector = IB_SA_EQ;
+#if defined(HAVE_VLAN_DEV_GET_EGRESS_QOS_MASK) && defined(HAVE_NETDEV_GET_PRIO_TC_MAP)
 	route->path_rec->sl = iboe_tos_to_sl(ndev, id_priv->tos);
+#elif defined(HAVE_NETDEV_GET_PRIO_TC_MAP)
+	route->path_rec->sl = netdev_get_prio_tc_map(
+			ndev->priv_flags & IFF_802_1Q_VLAN ?
+				vlan_dev_real_dev(ndev) : ndev,
+			rt_tos2priority(id_priv->tos));
+#else
+	route->path_rec->sl = id_priv->tos >> 5;
+#endif
 	route->path_rec->mtu = iboe_get_mtu(ndev->mtu);
 	route->path_rec->rate_selector = IB_SA_EQ;
 	route->path_rec->rate = iboe_get_rate(ndev);
@@ -2569,6 +2582,7 @@ static int cma_alloc_port(struct idr *ps, struct rdma_id_private *id_priv,
 			  unsigned short snum)
 {
 	struct rdma_bind_list *bind_list;
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(3, 9, 0))
 	int ret;
 
 	bind_list = kzalloc(sizeof *bind_list, GFP_KERNEL);
@@ -2586,6 +2600,35 @@ static int cma_alloc_port(struct idr *ps, struct rdma_id_private *id_priv,
 err:
 	kfree(bind_list);
 	return ret == -ENOSPC ? -EADDRNOTAVAIL : ret;
+#else
+	int port, ret;
+
+	bind_list = kzalloc(sizeof *bind_list, GFP_KERNEL);
+	if (!bind_list)
+		return -ENOMEM;
+
+	do {
+		ret = idr_get_new_above(ps, bind_list, snum, &port);
+	} while ((ret == -EAGAIN) && idr_pre_get(ps, GFP_KERNEL));
+
+	if (ret)
+		goto err1;
+
+	if (port != snum) {
+		ret = -EADDRNOTAVAIL;
+		goto err2;
+	}
+
+	bind_list->ps = ps;
+	bind_list->port = (unsigned short) port;
+	cma_bind_port(bind_list, id_priv);
+	return 0;
+err2:
+	idr_remove(ps, port);
+err1:
+	kfree(bind_list);
+	return ret;
+#endif
 }
 
 static int cma_alloc_any_port(struct idr *ps, struct rdma_id_private *id_priv)
@@ -2594,7 +2637,11 @@ static int cma_alloc_any_port(struct idr *ps, struct rdma_id_private *id_priv)
 	int low, high, remaining;
 	unsigned int rover;
 
+#ifdef HAVE_INET_GET_LOCAL_PORT_RANGE_3_PARAMS
 	inet_get_local_port_range(&init_net, &low, &high);
+#else
+	inet_get_local_port_range(&low, &high);
+#endif
 	remaining = (high - low) + 1;
 	rover = prandom_u32() % remaining + low;
 retry:
@@ -2630,9 +2677,16 @@ static int cma_check_port(struct rdma_bind_list *bind_list,
 {
 	struct rdma_id_private *cur_id;
 	struct sockaddr *addr, *cur_addr;
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,9,0))
+	struct hlist_node *node;
+#endif
 
 	addr = cma_src_addr(id_priv);
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,9,0))
+	hlist_for_each_entry(cur_id, node, &bind_list->owners, node) {
+#else
 	hlist_for_each_entry(cur_id, &bind_list->owners, node) {
+#endif
 		if (id_priv == cur_id)
 			continue;
 
@@ -3532,7 +3586,11 @@ static int cma_join_ib_multicast(struct rdma_id_private *id_priv,
 						id_priv->id.port_num, &rec,
 						comp_mask, GFP_KERNEL,
 						cma_ib_mc_handler, mc);
+#ifdef HAVE_PTR_ERR_OR_ZERO
 	return PTR_ERR_OR_ZERO(mc->multicast.ib);
+#else
+	return PTR_RET(mc->multicast.ib);
+#endif
 }
 
 static void iboe_mcast_work_handler(struct work_struct *work)
@@ -3780,9 +3838,15 @@ static int cma_netdev_change(struct net_device *ndev, struct rdma_id_private *id
 }
 
 static int cma_netdev_callback(struct notifier_block *self, unsigned long event,
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(3, 11, 0))
 			       void *ptr)
 {
 	struct net_device *ndev = netdev_notifier_info_to_dev(ptr);
+#else
+			       void *ctx)
+{
+	struct net_device *ndev = (struct net_device *)ctx;
+#endif
 	struct cma_device *cma_dev;
 	struct rdma_id_private *id_priv;
 	int ret = NOTIFY_DONE;
diff --git a/drivers/infiniband/core/fmr_pool.c b/drivers/infiniband/core/fmr_pool.c
index xxxxxxx..xxxxxxx xxxxxx
--- a/drivers/infiniband/core/fmr_pool.c
+++ b/drivers/infiniband/core/fmr_pool.c
@@ -118,13 +118,20 @@ static inline struct ib_pool_fmr *ib_fmr_cache_lookup(struct ib_fmr_pool *pool,
 {
 	struct hlist_head *bucket;
 	struct ib_pool_fmr *fmr;
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,9,0))
+	struct hlist_node *pos;
+#endif
 
 	if (!pool->cache_bucket)
 		return NULL;
 
 	bucket = pool->cache_bucket + ib_fmr_hash(*page_list);
 
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,9,0))
+	hlist_for_each_entry(fmr, pos, bucket, cache_node)
+#else
 	hlist_for_each_entry(fmr, bucket, cache_node)
+#endif
 		if (io_virtual_address == fmr->io_virtual_address &&
 		    page_list_len      == fmr->page_list_len      &&
 		    !memcmp(page_list, fmr->page_list,
diff --git a/drivers/infiniband/core/iwcm.c b/drivers/infiniband/core/iwcm.c
index xxxxxxx..xxxxxxx xxxxxx
--- a/drivers/infiniband/core/iwcm.c
+++ b/drivers/infiniband/core/iwcm.c
@@ -80,6 +80,14 @@ static struct ctl_table iwcm_ctl_table[] = {
 	{ }
 };
 
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3,5,0)
+static struct ctl_path iwcm_ctl_path[] = {
+    { .procname = "net" },
+    { .procname = "iw_cm" },
+    { }
+};
+#endif
+
 /*
  * The following services provide a mechanism for pre-allocating iwcm_work
  * elements.  The design pre-allocates them  based on the cm_id type:
@@ -1048,8 +1056,12 @@ static int __init iw_cm_init(void)
 	if (!iwcm_wq)
 		return -ENOMEM;
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,5,0)
 	iwcm_ctl_table_hdr = register_net_sysctl(&init_net, "net/iw_cm",
 						 iwcm_ctl_table);
+#else
+	iwcm_ctl_table_hdr = register_sysctl_paths(iwcm_ctl_path, iwcm_ctl_table);
+#endif
 	if (!iwcm_ctl_table_hdr) {
 		pr_err("iw_cm: couldn't register sysctl paths\n");
 		destroy_workqueue(iwcm_wq);
@@ -1061,7 +1073,11 @@ static int __init iw_cm_init(void)
 
 static void __exit iw_cm_cleanup(void)
 {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,5,0)
 	unregister_net_sysctl_table(iwcm_ctl_table_hdr);
+#else
+	unregister_sysctl_table(iwcm_ctl_table_hdr);
+#endif
 	destroy_workqueue(iwcm_wq);
 }
 
diff --git a/drivers/infiniband/core/iwpm_util.c b/drivers/infiniband/core/iwpm_util.c
index xxxxxxx..xxxxxxx xxxxxx
--- a/drivers/infiniband/core/iwpm_util.c
+++ b/drivers/infiniband/core/iwpm_util.c
@@ -131,6 +131,9 @@ int iwpm_remove_mapinfo(struct sockaddr_storage *local_sockaddr,
 	struct hlist_node *tmp_hlist_node;
 	struct hlist_head *hash_bucket_head;
 	struct iwpm_mapping_info *map_info = NULL;
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,9,0))
+	struct hlist_node *node;
+#endif
 	unsigned long flags;
 	int ret = -EINVAL;
 
@@ -139,9 +142,13 @@ int iwpm_remove_mapinfo(struct sockaddr_storage *local_sockaddr,
 		hash_bucket_head = get_hash_bucket_head(
 					local_sockaddr,
 					mapped_local_addr);
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,9,0))
+		hlist_for_each_entry_safe(map_info, node, tmp_hlist_node,
+					hash_bucket_head, hlist_node) {
+#else
 		hlist_for_each_entry_safe(map_info, tmp_hlist_node,
 					hash_bucket_head, hlist_node) {
-
+#endif
 			if (!iwpm_compare_sockaddr(&map_info->mapped_sockaddr,
 						mapped_local_addr)) {
 
@@ -161,15 +168,22 @@ static void free_hash_bucket(void)
 {
 	struct hlist_node *tmp_hlist_node;
 	struct iwpm_mapping_info *map_info;
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,9,0))
+	struct hlist_node *node;
+#endif
 	unsigned long flags;
 	int i;
 
 	/* remove all the mapinfo data from the list */
 	spin_lock_irqsave(&iwpm_mapinfo_lock, flags);
 	for (i = 0; i < IWPM_HASH_BUCKET_SIZE; i++) {
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,9,0))
+		hlist_for_each_entry_safe(map_info, node, tmp_hlist_node,
+			&iwpm_hash_bucket[i], hlist_node) {
+#else
 		hlist_for_each_entry_safe(map_info, tmp_hlist_node,
 			&iwpm_hash_bucket[i], hlist_node) {
-
+#endif
 				hlist_del_init(&map_info->hlist_node);
 				kfree(map_info);
 			}
@@ -498,6 +512,9 @@ int iwpm_send_mapinfo(u8 nl_client, int iwpm_pid)
 	struct iwpm_mapping_info *map_info;
 	struct sk_buff *skb = NULL;
 	struct nlmsghdr *nlh;
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,9,0))
+	struct hlist_node *node;
+#endif
 	int skb_num = 0, mapping_num = 0;
 	int i = 0, nlmsg_bytes = 0;
 	unsigned long flags;
@@ -513,8 +530,13 @@ int iwpm_send_mapinfo(u8 nl_client, int iwpm_pid)
 	skb_num++;
 	spin_lock_irqsave(&iwpm_mapinfo_lock, flags);
 	for (i = 0; i < IWPM_HASH_BUCKET_SIZE; i++) {
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3,9,0))
+		hlist_for_each_entry(map_info, node,
+			&iwpm_hash_bucket[i], hlist_node) {
+#else
 		hlist_for_each_entry(map_info, &iwpm_hash_bucket[i],
 				     hlist_node) {
+#endif
 			if (map_info->nl_client != nl_client)
 				continue;
 			nlh = NULL;
diff --git a/drivers/infiniband/core/mad.c b/drivers/infiniband/core/mad.c
index xxxxxxx..xxxxxxx xxxxxx
--- a/drivers/infiniband/core/mad.c
+++ b/drivers/infiniband/core/mad.c
@@ -34,6 +34,9 @@
  *
  */
 
+#ifdef pr_fmt
+#undef pr_fmt
+#endif
 #define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
 
 #include <linux/dma-mapping.h>
diff --git a/drivers/infiniband/core/netlink.c b/drivers/infiniband/core/netlink.c
index xxxxxxx..xxxxxxx xxxxxx
--- a/drivers/infiniband/core/netlink.c
+++ b/drivers/infiniband/core/netlink.c
@@ -30,6 +30,9 @@
  * SOFTWARE.
  */
 
+#ifdef pr_fmt
+#undef pr_fmt
+#endif
 #define pr_fmt(fmt) "%s:%s: " fmt, KBUILD_MODNAME, __func__
 
 #include <linux/export.h>
@@ -152,11 +155,22 @@ static int ibnl_rcv_msg(struct sk_buff *skb, struct nlmsghdr *nlh)
 				return -EINVAL;
 
 			{
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,4,0) || defined(CONFIG_COMPAT_NETLINK_3_7)
 				struct netlink_dump_control c = {
 					.dump = client->cb_table[op].dump,
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,7,0) || defined(CONFIG_COMPAT_NETLINK_3_7)
 					.module = client->cb_table[op].module,
+#endif
 				};
+#endif /* (LINUX_VERSION_CODE >= KERNEL_VERSION(3,4,0)) */
+
+#if defined(HAVE_NETLINK_DUMP_START_6P) || defined(HAVE_NETLINK_DUMP_START_5P)
+				return netlink_dump_start(nls, skb, nlh,
+							  client->cb_table[op].dump,
+							  NULL, 0);
+#else
 				return netlink_dump_start(nls, skb, nlh, &c);
+#endif
 			}
 		}
 	}
@@ -188,11 +202,20 @@ EXPORT_SYMBOL(ibnl_multicast);
 
 int __init ibnl_init(void)
 {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,6,0)
 	struct netlink_kernel_cfg cfg = {
 		.input	= ibnl_rcv,
 	};
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,7,0)
 	nls = netlink_kernel_create(&init_net, NETLINK_RDMA, &cfg);
+#else
+	nls = netlink_kernel_create(&init_net, NETLINK_RDMA, THIS_MODULE, &cfg);
+#endif
+#else /* LINUX_VERSION_CODE >= KERNEL_VERSION(3,6,0) */
+	nls = netlink_kernel_create(&init_net, NETLINK_RDMA, 0, ibnl_rcv,
+				    NULL, THIS_MODULE);
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(3,6,0) */
 	if (!nls) {
 		pr_warn("Failed to create netlink socket\n");
 		return -ENOMEM;
diff --git a/drivers/infiniband/core/roce_gid_cache.c b/drivers/infiniband/core/roce_gid_cache.c
index xxxxxxx..xxxxxxx xxxxxx
--- a/drivers/infiniband/core/roce_gid_cache.c
+++ b/drivers/infiniband/core/roce_gid_cache.c
@@ -400,7 +400,11 @@ static int get_netdev_from_ifindex(struct net *net, int if_index,
 {
 	if (if_index && net) {
 		rcu_read_lock();
+#ifdef HAVE_DEV_GET_BY_INDEX_RCU
 		gid_attr_val->ndev = dev_get_by_index_rcu(net, if_index);
+#else
+		gid_attr_val->ndev = __dev_get_by_index(net, if_index);
+#endif
 		rcu_read_unlock();
 		if (gid_attr_val->ndev)
 			return GID_ATTR_FIND_MASK_NETDEV;
diff --git a/drivers/infiniband/core/roce_gid_mgmt.c b/drivers/infiniband/core/roce_gid_mgmt.c
index xxxxxxx..xxxxxxx xxxxxx
--- a/drivers/infiniband/core/roce_gid_mgmt.c
+++ b/drivers/infiniband/core/roce_gid_mgmt.c
@@ -142,16 +142,22 @@ enum bonding_slave_state {
 static enum bonding_slave_state is_eth_active_slave_of_bonding(struct net_device *idev,
 							       struct net_device *upper)
 {
+#ifdef HAVE_BONDING_H
 	if (upper && IS_NETDEV_BONDING_MASTER(upper)) {
 		struct net_device *pdev;
 
+#ifdef HAVE_BOND_OPTION_ACTIVE_SLAVE_GET_SAFE
+		pdev = bond_option_active_slave_get_safe(netdev_priv(upper));
+#else
 		rcu_read_lock();
 		pdev = bond_option_active_slave_get_rcu(netdev_priv(upper));
 		rcu_read_unlock();
+#endif
 		if (pdev)
 			return idev == pdev ? BONDING_SLAVE_STATE_ACTIVE :
 				BONDING_SLAVE_STATE_INACTIVE;
 	}
+#endif
 
 	return BONDING_SLAVE_STATE_NA;
 }
@@ -159,15 +165,23 @@ static enum bonding_slave_state is_eth_active_slave_of_bonding(struct net_device
 static bool is_upper_dev_rcu(struct net_device *dev, struct net_device *upper)
 {
 	struct net_device *_upper = NULL;
+#ifdef HAVE_NETDEV_FOR_EACH_ALL_UPPER_DEV_RCU
 	struct list_head *iter;
+#endif
 
+#if defined(HAVE_NETDEV_FOR_EACH_ALL_UPPER_DEV_RCU) || defined(HAVE_NETDEV_MASTER_UPPER_DEV_GET_RCU)
 	rcu_read_lock();
+#ifdef HAVE_NETDEV_FOR_EACH_ALL_UPPER_DEV_RCU
 	netdev_for_each_all_upper_dev_rcu(dev, _upper, iter) {
 		if (_upper == upper)
 			break;
 	}
+#else
+	_upper = netdev_master_upper_dev_get_rcu(dev);
+#endif
 
 	rcu_read_unlock();
+#endif
 	return _upper == upper;
 }
 
@@ -212,11 +226,15 @@ static int is_eth_port_inactive_slave(struct ib_device *ib_dev, u8 port,
 	if (!idev)
 		return 0;
 
+#ifdef HAVE_NETDEV_MASTER_UPPER_DEV_GET_RCU
 	rcu_read_lock();
+#endif
 	mdev = netdev_master_upper_dev_get_rcu(idev);
 	res = is_eth_active_slave_of_bonding(idev, mdev) ==
 		BONDING_SLAVE_STATE_INACTIVE;
+#ifdef HAVE_NETDEV_MASTER_UPPER_DEV_GET_RCU
 	rcu_read_unlock();
+#endif
 
 	return res;
 }
@@ -379,7 +397,11 @@ static void enum_netdev_ipv6_ips(struct ib_device *ib_dev,
 		return;
 
 	read_lock_bh(&in6_dev->lock);
+#ifdef HAVE_INET6_IF_LIST
 	list_for_each_entry(ifp, &in6_dev->addr_list, if_list) {
+#else
+	for (ifp=in6_dev->addr_list; ifp; ifp=ifp->if_next) {
+#endif
 		struct sin6_list *entry = kzalloc(sizeof(*entry), GFP_ATOMIC);
 
 		if (!entry) {
@@ -426,34 +448,50 @@ static void del_netdev_ips(struct ib_device *ib_dev, u8 port,
 	roce_del_all_netdev_gids(ib_dev, port, ndev);
 }
 
+#ifdef HAVE_NETDEV_CHANGEUPPER
 static void del_netdev_upper_ips(struct ib_device *ib_dev, u8 port,
 				 struct net_device *idev, void *cookie)
 {
+#if defined(HAVE_NETDEV_FOR_EACH_ALL_UPPER_DEV_RCU) || defined(HAVE_NETDEV_MASTER_UPPER_DEV_GET_RCU)
 	if (idev) {
 		struct upper_list {
 			struct list_head list;
 			struct net_device *upper;
 		};
-		struct net_device *upper;
+		struct net_device *upper = NULL;
 		struct list_head *iter;
 		struct upper_list *upper_iter;
 		struct upper_list *upper_temp;
 		LIST_HEAD(upper_list);
 
 		rcu_read_lock();
+#ifdef HAVE_NETDEV_FOR_EACH_ALL_UPPER_DEV_RCU
 		netdev_for_each_all_upper_dev_rcu(idev, upper, iter) {
+#else
+		/* We only support the bond itself here, vlans will be
+		 * problematic */
+		upper = netdev_master_upper_dev_get_rcu(idev);
+		if (upper) {
+#endif
 			struct upper_list *entry = kmalloc(sizeof(*entry),
 							   GFP_ATOMIC);
 
 			if (!entry) {
 				pr_info("roce_gid_mgmt: couldn't allocate entry to delete ndev\n");
+#ifdef HAVE_NETDEV_FOR_EACH_ALL_UPPER_DEV_RCU
 				continue;
+#else
+				goto unlock;
+#endif
 			}
 
 			list_add_tail(&entry->list, &upper_list);
 			dev_hold(upper);
 			entry->upper = upper;
 		}
+#ifdef HAVE_NETDEV_MASTER_UPPER_DEV_GET_RCU
+unlock:
+#endif
 		rcu_read_unlock();
 
 		list_for_each_entry_safe(upper_iter, upper_temp, &upper_list,
@@ -465,7 +503,9 @@ static void del_netdev_upper_ips(struct ib_device *ib_dev, u8 port,
 			kfree(upper_iter);
 		}
 	}
+#endif
 }
+#endif
 
 static void del_netdev_default_ips(struct ib_device *ib_dev, u8 port,
 				   struct net_device *idev, void *cookie)
@@ -486,8 +526,10 @@ static int netdevice_event(struct notifier_block *this, unsigned long event,
 		.cb = del_netdev_default_ips, .filter = is_eth_port_inactive_slave};
 	static const struct netdev_event_work_cmd bonding_event_ips_del_cmd = {
 		.cb = del_netdev_ips, .filter = bonding_slaves_filter};
+#ifdef HAVE_NETDEV_CHANGEUPPER
 	static const struct netdev_event_work_cmd upper_ips_del_cmd = {
 		.cb = del_netdev_upper_ips, .filter = pass_all_filter};
+#endif
 	struct net_device *ndev = netdev_notifier_info_to_dev(ptr);
 	struct netdev_event_work *ndev_work;
 	struct netdev_event_work_cmd cmds[ROCE_NETDEV_CALLBACK_SZ] = { {NULL} };
@@ -515,10 +557,12 @@ static int netdevice_event(struct notifier_block *this, unsigned long event,
 		cmds[1] = add_cmd;
 		break;
 
+#ifdef HAVE_NETDEV_CHANGEUPPER
 	case NETDEV_CHANGEUPPER:
 		cmds[0] = upper_ips_del_cmd;
 		cmds[1] = add_cmd;
 		break;
+#endif
 
 	case NETDEV_BONDING_FAILOVER:
 		cmds[0] = bonding_event_ips_del_cmd;
@@ -745,6 +789,10 @@ void __exit roce_gid_mgmt_cleanup(void)
 	 * so no issue with remaining hardware contexts.
 	 */
 	synchronize_rcu();
+#ifdef HAVE_DRAIN_WORKQUEUE
 	drain_workqueue(roce_gid_mgmt_wq);
+#endif
+	/* Old implementaion of destroy_workqueue will drain the queue and
+	 * destroy it. */
 	destroy_workqueue(roce_gid_mgmt_wq);
 }
diff --git a/drivers/infiniband/core/sa_query.c b/drivers/infiniband/core/sa_query.c
index xxxxxxx..xxxxxxx xxxxxx
--- a/drivers/infiniband/core/sa_query.c
+++ b/drivers/infiniband/core/sa_query.c
@@ -42,7 +42,11 @@
 #include <linux/kref.h>
 #include <linux/idr.h>
 #include <linux/workqueue.h>
+#ifdef HAVE_UAPI_LINUX_IF_ETHER_H
 #include <uapi/linux/if_ether.h>
+#else
+#include <linux/if_ether.h>
+#endif
 #include <rdma/ib_pack.h>
 #include <rdma/ib_cache.h>
 #include "sa.h"
@@ -615,10 +619,13 @@ static void init_mad(struct ib_sa_mad *mad, struct ib_mad_agent *agent)
 
 static int send_mad(struct ib_sa_query *query, int timeout_ms, gfp_t gfp_mask)
 {
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(3, 9, 0))
 	bool preload = !!(gfp_mask & __GFP_WAIT);
+#endif
 	unsigned long flags;
 	int ret, id;
 
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(3, 9, 0))
 	if (preload)
 		idr_preload(gfp_mask);
 	spin_lock_irqsave(&idr_lock, flags);
@@ -630,6 +637,18 @@ static int send_mad(struct ib_sa_query *query, int timeout_ms, gfp_t gfp_mask)
 		idr_preload_end();
 	if (id < 0)
 		return id;
+#else
+retry:
+	if (!idr_pre_get(&query_idr, gfp_mask))
+		return -ENOMEM;
+	spin_lock_irqsave(&idr_lock, flags);
+	ret = idr_get_new(&query_idr, query, &id);
+	spin_unlock_irqrestore(&idr_lock, flags);
+	if (ret == -EAGAIN)
+		goto retry;
+	if (ret)
+		return ret;
+#endif
 
 	query->mad_buf->timeout_ms  = timeout_ms;
 	query->mad_buf->context[0] = query;
diff --git a/drivers/infiniband/core/ucm.c b/drivers/infiniband/core/ucm.c
index xxxxxxx..xxxxxxx xxxxxx
--- a/drivers/infiniband/core/ucm.c
+++ b/drivers/infiniband/core/ucm.c
@@ -176,6 +176,9 @@ static void ib_ucm_cleanup_events(struct ib_ucm_context *ctx)
 static struct ib_ucm_context *ib_ucm_ctx_alloc(struct ib_ucm_file *file)
 {
 	struct ib_ucm_context *ctx;
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3, 9, 0))
+	int result;
+#endif
 
 	ctx = kzalloc(sizeof *ctx, GFP_KERNEL);
 	if (!ctx)
@@ -186,11 +189,26 @@ static struct ib_ucm_context *ib_ucm_ctx_alloc(struct ib_ucm_file *file)
 	ctx->file = file;
 	INIT_LIST_HEAD(&ctx->events);
 
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(3, 9, 0))
 	mutex_lock(&ctx_id_mutex);
 	ctx->id = idr_alloc(&ctx_id_table, ctx, 0, 0, GFP_KERNEL);
 	mutex_unlock(&ctx_id_mutex);
 	if (ctx->id < 0)
 		goto error;
+#else
+	do {
+		result = idr_pre_get(&ctx_id_table, GFP_KERNEL);
+		if (!result)
+			goto error;
+
+		mutex_lock(&ctx_id_mutex);
+		result = idr_get_new(&ctx_id_table, ctx, &ctx->id);
+		mutex_unlock(&ctx_id_mutex);
+	} while (result == -EAGAIN);
+
+	if (result)
+		goto error;
+#endif
 
 	list_add_tail(&ctx->file_list, &file->ctxs);
 	return ctx;
@@ -1321,8 +1339,16 @@ static void ib_ucm_remove_one(struct ib_device *device)
 	device_unregister(&ucm_dev->dev);
 }
 
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,34))
 static CLASS_ATTR_STRING(abi_version, S_IRUGO,
 			 __stringify(IB_USER_CM_ABI_VERSION));
+#else
+static ssize_t show_abi_version(struct class *class, char *buf)
+{
+	return sprintf(buf, "%d\n", IB_USER_CM_ABI_VERSION);
+}
+static CLASS_ATTR(abi_version, S_IRUGO, show_abi_version, NULL);
+#endif
 
 static int __init ib_ucm_init(void)
 {
@@ -1335,7 +1361,11 @@ static int __init ib_ucm_init(void)
 		goto error1;
 	}
 
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,34))
 	ret = class_create_file(&cm_class, &class_attr_abi_version.attr);
+#else
+	ret = class_create_file(&cm_class, &class_attr_abi_version);
+#endif
 	if (ret) {
 		printk(KERN_ERR "ucm: couldn't create abi_version attribute\n");
 		goto error2;
@@ -1349,7 +1379,11 @@ static int __init ib_ucm_init(void)
 	return 0;
 
 error3:
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,34))
 	class_remove_file(&cm_class, &class_attr_abi_version.attr);
+#else
+	class_remove_file(&cm_class, &class_attr_abi_version);
+#endif
 error2:
 	unregister_chrdev_region(IB_UCM_BASE_DEV, IB_UCM_MAX_DEVICES);
 error1:
@@ -1359,7 +1393,11 @@ error1:
 static void __exit ib_ucm_cleanup(void)
 {
 	ib_unregister_client(&ucm_client);
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,34))
 	class_remove_file(&cm_class, &class_attr_abi_version.attr);
+#else
+	class_remove_file(&cm_class, &class_attr_abi_version);
+#endif
 	unregister_chrdev_region(IB_UCM_BASE_DEV, IB_UCM_MAX_DEVICES);
 	if (overflow_maj)
 		unregister_chrdev_region(overflow_maj, IB_UCM_MAX_DEVICES);
diff --git a/drivers/infiniband/core/ucma.c b/drivers/infiniband/core/ucma.c
index xxxxxxx..xxxxxxx xxxxxx
--- a/drivers/infiniband/core/ucma.c
+++ b/drivers/infiniband/core/ucma.c
@@ -56,6 +56,7 @@ MODULE_LICENSE("Dual BSD/GPL");
 
 static unsigned int max_backlog = 1024;
 
+#ifndef CONFIG_SYSCTL_SYSCALL_CHECK
 static struct ctl_table_header *ucma_ctl_table_hdr;
 static struct ctl_table ucma_ctl_table[] = {
 	{
@@ -67,6 +68,14 @@ static struct ctl_table ucma_ctl_table[] = {
 	},
 	{ }
 };
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3,5,0)
+static struct ctl_path ucma_ctl_path[] = {
+	{ .procname = "net" },
+	{ .procname = "rdma_ucm" },
+	{ }
+};
+#endif
+#endif
 
 struct ucma_file {
 	struct mutex		mut;
@@ -184,6 +193,9 @@ static void ucma_close_id(struct work_struct *work)
 static struct ucma_context *ucma_alloc_ctx(struct ucma_file *file)
 {
 	struct ucma_context *ctx;
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3, 9, 0))
+	int ret;
+#endif
 
 	ctx = kzalloc(sizeof(*ctx), GFP_KERNEL);
 	if (!ctx)
@@ -195,11 +207,26 @@ static struct ucma_context *ucma_alloc_ctx(struct ucma_file *file)
 	INIT_LIST_HEAD(&ctx->mc_list);
 	ctx->file = file;
 
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3, 9, 0))
+	do {
+		ret = idr_pre_get(&ctx_idr, GFP_KERNEL);
+		if (!ret)
+			goto error;
+
+		mutex_lock(&mut);
+		ret = idr_get_new(&ctx_idr, ctx, &ctx->id);
+		mutex_unlock(&mut);
+	} while (ret == -EAGAIN);
+
+	if (ret)
+		goto error;
+#else
 	mutex_lock(&mut);
 	ctx->id = idr_alloc(&ctx_idr, ctx, 0, 0, GFP_KERNEL);
 	mutex_unlock(&mut);
 	if (ctx->id < 0)
 		goto error;
+#endif
 
 	list_add_tail(&ctx->list, &file->ctx_list);
 	return ctx;
@@ -212,16 +239,34 @@ error:
 static struct ucma_multicast* ucma_alloc_multicast(struct ucma_context *ctx)
 {
 	struct ucma_multicast *mc;
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3, 9, 0))
+	int ret;
+#endif
 
 	mc = kzalloc(sizeof(*mc), GFP_KERNEL);
 	if (!mc)
 		return NULL;
 
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(3, 9, 0))
+	do {
+		ret = idr_pre_get(&multicast_idr, GFP_KERNEL);
+		if (!ret)
+			goto error;
+
+		mutex_lock(&mut);
+		ret = idr_get_new(&multicast_idr, mc, &mc->id);
+		mutex_unlock(&mut);
+	} while (ret == -EAGAIN);
+
+	if (ret)
+		goto error;
+#else
 	mutex_lock(&mut);
 	mc->id = idr_alloc(&multicast_idr, mc, 0, 0, GFP_KERNEL);
 	mutex_unlock(&mut);
 	if (mc->id < 0)
 		goto error;
+#endif
 
 	mc->ctx = ctx;
 	list_add_tail(&mc->list, &ctx->mc_list);
@@ -1508,7 +1553,11 @@ static ssize_t ucma_migrate_id(struct ucma_file *new_file,
 	struct rdma_ucm_migrate_id cmd;
 	struct rdma_ucm_migrate_resp resp;
 	struct ucma_context *ctx;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,7,0)
 	struct fd f;
+#else
+	struct file *filp;
+#endif
 	struct ucma_file *cur_file;
 	int ret = 0;
 
@@ -1516,12 +1565,21 @@ static ssize_t ucma_migrate_id(struct ucma_file *new_file,
 		return -EFAULT;
 
 	/* Get current fd to protect against it being closed */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,7,0)
 	f = fdget(cmd.fd);
 	if (!f.file)
+#else
+	filp = fget(cmd.fd);
+	if (!filp)
+#endif
 		return -ENOENT;
 
 	/* Validate current fd and prevent destruction of id. */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,7,0)
 	ctx = ucma_get_ctx(f.file->private_data, cmd.id);
+#else
+	ctx = ucma_get_ctx(filp->private_data, cmd.id);
+#endif
 	if (IS_ERR(ctx)) {
 		ret = PTR_ERR(ctx);
 		goto file_put;
@@ -1555,7 +1613,11 @@ response:
 
 	ucma_put_ctx(ctx);
 file_put:
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,7,0)
 	fdput(f);
+#else
+	fput(filp);
+#endif
 	return ret;
 }
 
@@ -1729,15 +1791,23 @@ static int __init ucma_init(void)
 		goto err1;
 	}
 
+#ifndef CONFIG_SYSCTL_SYSCALL_CHECK
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,5,0)
 	ucma_ctl_table_hdr = register_net_sysctl(&init_net, "net/rdma_ucm", ucma_ctl_table);
+#else
+	ucma_ctl_table_hdr = register_sysctl_paths(ucma_ctl_path, ucma_ctl_table);
+#endif
 	if (!ucma_ctl_table_hdr) {
 		printk(KERN_ERR "rdma_ucm: couldn't register sysctl paths\n");
 		ret = -ENOMEM;
 		goto err2;
 	}
+#endif
 	return 0;
+#ifndef CONFIG_SYSCTL_SYSCALL_CHECK
 err2:
 	device_remove_file(ucma_misc.this_device, &dev_attr_abi_version);
+#endif
 err1:
 	misc_deregister(&ucma_misc);
 	return ret;
@@ -1745,7 +1815,13 @@ err1:
 
 static void __exit ucma_cleanup(void)
 {
+#ifndef CONFIG_SYSCTL_SYSCALL_CHECK
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,5,0)
 	unregister_net_sysctl_table(ucma_ctl_table_hdr);
+#else
+	unregister_sysctl_table(ucma_ctl_table_hdr);
+#endif
+#endif
 	device_remove_file(ucma_misc.this_device, &dev_attr_abi_version);
 	misc_deregister(&ucma_misc);
 	idr_destroy(&ctx_idr);
diff --git a/drivers/infiniband/core/umem.c b/drivers/infiniband/core/umem.c
index xxxxxxx..xxxxxxx xxxxxx
--- a/drivers/infiniband/core/umem.c
+++ b/drivers/infiniband/core/umem.c
@@ -61,7 +61,11 @@ static void umem_vma_open(struct vm_area_struct *area)
 	with mm->mmap_sem held for writing.
 	*/
 	if (current->mm)
+#ifdef HAVE_PINNED_VM
 		current->mm->pinned_vm += ntotal_pages;
+#else
+		current->mm->locked_vm += ntotal_pages;
+#endif
 	return;
 }
 
@@ -81,7 +85,11 @@ static void umem_vma_close(struct vm_area_struct *area)
 	with mm->mmap_sem held for writing.
 	*/
 	if (current->mm)
+#ifdef HAVE_PINNED_VM
 		current->mm->pinned_vm -= ntotal_pages;
+#else
+		current->mm->locked_vm -= ntotal_pages;
+#endif
 	return;
 
 }
@@ -116,7 +124,11 @@ int ib_umem_map_to_vma(struct ib_umem *umem,
 	with mm->mmap_sem held for writing.
 	No need to lock.
 	*/
+#ifdef HAVE_PINNED_VM
 	locked = ntotal_pages + current->mm->pinned_vm;
+#else
+	locked = ntotal_pages + current->mm->locked_vm;
+#endif
 	lock_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;
 
 	if ((locked > lock_limit) && !capable(CAP_IPC_LOCK))
@@ -145,7 +157,11 @@ int ib_umem_map_to_vma(struct ib_umem *umem,
 end:
 	/* We expect to have enough pages   */
 	if (vma_entry_number >= ntotal_pages) {
+#ifdef HAVE_PINNED_VM
 		current->mm->pinned_vm = locked;
+#else
+		current->mm->locked_vm = locked;
+#endif
 		vma->vm_ops =  &umem_vm_ops;
 		return 0;
 	}
@@ -186,7 +202,11 @@ static void ib_cmem_release(struct kref *ref)
 	counter not relevant any more.*/
 	if (current->mm) {
 		ntotal_pages = PAGE_ALIGN(cmem->length) >> PAGE_SHIFT;
+#ifdef HAVE_PINNED_VM
 		current->mm->pinned_vm -= ntotal_pages;
+#else
+		current->mm->locked_vm -= ntotal_pages;
+#endif
 	}
 	kfree(cmem);
 
@@ -344,7 +364,11 @@ struct ib_cmem *ib_cmem_alloc_contiguous_pages(struct ib_ucontext *context,
 	with mm->mmap_sem held for writing.
 	No need to lock
 	 */
+#ifdef HAVE_PINNED_VM
 	locked     = ntotal_pages + current->mm->pinned_vm;
+#else
+	locked     = ntotal_pages + current->mm->locked_vm;
+#endif
 	lock_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;
 
 	if ((locked > lock_limit) && !capable(CAP_IPC_LOCK))
@@ -386,7 +410,11 @@ struct ib_cmem *ib_cmem_alloc_contiguous_pages(struct ib_ucontext *context,
 
 	cmem->length = total_size;
 
+#ifdef HAVE_PINNED_VM
 	current->mm->pinned_vm = locked;
+#else
+	current->mm->locked_vm = locked;
+#endif
 	return cmem;
 
 err_alloc:
@@ -678,7 +706,11 @@ struct ib_umem *ib_umem_get_ex(struct ib_ucontext *context, unsigned long addr,
 
 	down_write(&current->mm->mmap_sem);
 
+#ifdef HAVE_PINNED_VM
 	locked     = npages + current->mm->pinned_vm;
+#else
+	locked     = npages + current->mm->locked_vm;
+#endif
 	lock_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;
 
 	if ((locked > lock_limit) && !capable(CAP_IPC_LOCK)) {
@@ -744,7 +776,11 @@ out:
 		put_pid(umem->pid);
 		kfree(umem);
 	} else
+#ifdef HAVE_PINNED_VM
 		current->mm->pinned_vm = locked;
+#else
+		current->mm->locked_vm = locked;
+#endif
 
 	up_write(&current->mm->mmap_sem);
 	if (vma_list)
@@ -767,7 +803,11 @@ static void ib_umem_account(struct work_struct *work)
 	struct ib_umem *umem = container_of(work, struct ib_umem, work);
 
 	down_write(&umem->mm->mmap_sem);
+#ifdef HAVE_PINNED_VM
 	umem->mm->pinned_vm -= umem->diff;
+#else
+	umem->mm->locked_vm -= umem->diff;
+#endif
 	up_write(&umem->mm->mmap_sem);
 	mmput(umem->mm);
 	kfree(umem);
@@ -827,7 +867,11 @@ void ib_umem_release(struct ib_umem *umem)
 	} else
 		down_write(&mm->mmap_sem);
 
+#ifdef HAVE_PINNED_VM
 	mm->pinned_vm -= diff;
+#else
+	mm->locked_vm -= diff;
+#endif
 	up_write(&mm->mmap_sem);
 	mmput(mm);
 out:
diff --git a/drivers/infiniband/core/user_mad.c b/drivers/infiniband/core/user_mad.c
index xxxxxxx..xxxxxxx xxxxxx
--- a/drivers/infiniband/core/user_mad.c
+++ b/drivers/infiniband/core/user_mad.c
@@ -33,6 +33,9 @@
  * SOFTWARE.
  */
 
+#ifdef pr_fmt
+#undef pr_fmt
+#endif
 #define pr_fmt(fmt) "user_mad: " fmt
 
 #include <linux/module.h>
@@ -1117,8 +1120,16 @@ static ssize_t show_port(struct device *dev, struct device_attribute *attr,
 }
 static DEVICE_ATTR(port, S_IRUGO, show_port, NULL);
 
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,34))
 static CLASS_ATTR_STRING(abi_version, S_IRUGO,
 			 __stringify(IB_USER_MAD_ABI_VERSION));
+#else
+static ssize_t show_abi_version(struct class *class, char *buf)
+{
+	return sprintf(buf, "%d\n", IB_USER_MAD_ABI_VERSION);
+}
+static CLASS_ATTR(abi_version, S_IRUGO, show_abi_version, NULL);
+#endif
 
 static dev_t overflow_maj;
 static DECLARE_BITMAP(overflow_map, IB_UMAD_MAX_PORTS);
@@ -1328,7 +1339,11 @@ static void ib_umad_remove_one(struct ib_device *device)
 	kobject_put(&umad_dev->kobj);
 }
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,3,0)
 static char *umad_devnode(struct device *dev, umode_t *mode)
+#else
+static char *umad_devnode(struct device *dev, mode_t *mode)
+#endif
 {
 	return kasprintf(GFP_KERNEL, "infiniband/%s", dev_name(dev));
 }
@@ -1353,7 +1368,11 @@ static int __init ib_umad_init(void)
 
 	umad_class->devnode = umad_devnode;
 
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,34))
 	ret = class_create_file(umad_class, &class_attr_abi_version.attr);
+#else
+	ret = class_create_file(umad_class, &class_attr_abi_version);
+#endif
 	if (ret) {
 		pr_err("couldn't create abi_version attribute\n");
 		goto out_class;
diff --git a/drivers/infiniband/core/uverbs_cmd.c b/drivers/infiniband/core/uverbs_cmd.c
index xxxxxxx..xxxxxxx xxxxxx
--- a/drivers/infiniband/core/uverbs_cmd.c
+++ b/drivers/infiniband/core/uverbs_cmd.c
@@ -161,6 +161,7 @@ static int idr_add_uobj(struct idr *idr, struct ib_uobject *uobj)
 {
 	int ret;
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,9,0)
 	idr_preload(GFP_KERNEL);
 	spin_lock(&ib_uverbs_idr_lock);
 
@@ -172,6 +173,20 @@ static int idr_add_uobj(struct idr *idr, struct ib_uobject *uobj)
 	idr_preload_end();
 
 	return ret < 0 ? ret : 0;
+#else
+retry:
+	if (!idr_pre_get(idr, GFP_KERNEL))
+		return -ENOMEM;
+
+	spin_lock(&ib_uverbs_idr_lock);
+	ret = idr_get_new(idr, uobj, &uobj->id);
+	spin_unlock(&ib_uverbs_idr_lock);
+
+	if (ret == -EAGAIN)
+		goto retry;
+
+	return ret;
+#endif
 }
 
 void idr_remove_uobj(struct idr *idr, struct ib_uobject *uobj)
@@ -408,7 +423,11 @@ ssize_t ib_uverbs_get_context(struct ib_uverbs_file *file,
 
 	resp.num_comp_vectors = file->device->num_comp_vectors;
 
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(3, 11, 0))
 	ret = get_unused_fd_flags(O_CLOEXEC);
+#else
+	ret = get_unused_fd();
+#endif
 	if (ret < 0)
 		goto err_free;
 	resp.async_fd = ret;
@@ -832,7 +851,11 @@ ssize_t ib_uverbs_open_xrcd(struct ib_uverbs_file *file,
 	struct ib_udata			udata;
 	struct ib_uxrcd_object         *obj;
 	struct ib_xrcd                 *xrcd = NULL;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,7,0)
 	struct fd			f = {NULL, 0};
+#else
+	struct file                    *f = NULL;
+#endif
 	struct inode                   *inode = NULL;
 	int				ret = 0;
 	int				new_xrcd = 0;
@@ -851,6 +874,7 @@ ssize_t ib_uverbs_open_xrcd(struct ib_uverbs_file *file,
 
 	if (cmd.fd != -1) {
 		/* search for file descriptor */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,7,0)
 		f = fdget(cmd.fd);
 		if (!f.file) {
 			ret = -EBADF;
@@ -858,6 +882,19 @@ ssize_t ib_uverbs_open_xrcd(struct ib_uverbs_file *file,
 		}
 
 		inode = file_inode(f.file);
+#else
+		f = fget(cmd.fd);
+		if (!f) {
+			ret = -EBADF;
+			goto err_tree_mutex_unlock;
+		}
+
+		inode = f->f_dentry->d_inode;
+		if (!inode) {
+			ret = -EBADF;
+			goto err_tree_mutex_unlock;
+		}
+#endif
 		xrcd = find_xrcd(file->device, inode);
 		if (!xrcd && !(cmd.oflags & O_CREAT)) {
 			/* no file descriptor. Need CREATE flag */
@@ -922,8 +959,13 @@ ssize_t ib_uverbs_open_xrcd(struct ib_uverbs_file *file,
 		goto err_copy;
 	}
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,7,0)
 	if (f.file)
 		fdput(f);
+#else
+	if (f)
+		fput(f);
+#endif
 
 	mutex_lock(&file->mutex);
 	list_add_tail(&obj->uobject.list, &file->ucontext->xrcd_list);
@@ -952,8 +994,13 @@ err:
 	put_uobj_write(&obj->uobject);
 
 err_tree_mutex_unlock:
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,7,0)
 	if (f.file)
 		fdput(f);
+#else
+	if (f)
+		fput(f);
+#endif
 
 	mutex_unlock(&file->device->xrcd_tree_mutex);
 
@@ -1700,7 +1747,11 @@ ssize_t ib_uverbs_create_comp_channel(struct ib_uverbs_file *file,
 	if (copy_from_user(&cmd, buf, sizeof cmd))
 		return -EFAULT;
 
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(3, 11, 0))
 	ret = get_unused_fd_flags(O_CLOEXEC);
+#else
+	ret = get_unused_fd();
+#endif
 	if (ret < 0)
 		return ret;
 	resp.fd = ret;
diff --git a/drivers/infiniband/core/uverbs_main.c b/drivers/infiniband/core/uverbs_main.c
index xxxxxxx..xxxxxxx xxxxxx
--- a/drivers/infiniband/core/uverbs_main.c
+++ b/drivers/infiniband/core/uverbs_main.c
@@ -659,6 +659,7 @@ struct file *ib_uverbs_alloc_event_file(struct ib_uverbs_file *uverbs_file,
 struct ib_uverbs_event_file *ib_uverbs_lookup_comp_file(int fd)
 {
 	struct ib_uverbs_event_file *ev_file = NULL;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,7,0)
 	struct fd f = fdget(fd);
 
 	if (!f.file)
@@ -678,6 +679,29 @@ struct ib_uverbs_event_file *ib_uverbs_lookup_comp_file(int fd)
 out:
 	fdput(f);
 	return ev_file;
+#else
+	struct file *filp;
+	int fput_needed;
+
+	filp = fget_light(fd, &fput_needed);
+	if (!filp)
+		return NULL;
+
+	if (filp->f_op != &uverbs_event_fops)
+		goto out;
+
+	ev_file = filp->private_data;
+	if (ev_file->is_async) {
+		ev_file = NULL;
+		goto out;
+	}
+
+	kref_get(&ev_file->ref);
+
+out:
+	fput_light(filp, fput_needed);
+	return ev_file;
+#endif
 }
 
 enum {
@@ -1060,8 +1084,16 @@ static ssize_t show_dev_abi_version(struct device *device,
 }
 static DEVICE_ATTR(abi_version, S_IRUGO, show_dev_abi_version, NULL);
 
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,34))
 static CLASS_ATTR_STRING(abi_version, S_IRUGO,
 			 __stringify(IB_USER_VERBS_ABI_VERSION));
+#else
+static ssize_t show_abi_version(struct class *class, char *buf)
+{
+	return sprintf(buf, "%d\n", IB_USER_VERBS_ABI_VERSION);
+}
+static CLASS_ATTR(abi_version, S_IRUGO, show_abi_version, NULL);
+#endif
 
 static dev_t overflow_maj;
 static DECLARE_BITMAP(overflow_map, IB_UVERBS_MAX_DEVICES);
@@ -1278,7 +1310,11 @@ static void ib_uverbs_remove_one(struct ib_device *device)
 	}
 }
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,3,0)
 static char *uverbs_devnode(struct device *dev, umode_t *mode)
+#else
+static char *uverbs_devnode(struct device *dev, mode_t *mode)
+#endif
 {
 	if (mode)
 		*mode = 0666;
@@ -1305,7 +1341,11 @@ static int __init ib_uverbs_init(void)
 
 	uverbs_class->devnode = uverbs_devnode;
 
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,34))
 	ret = class_create_file(uverbs_class, &class_attr_abi_version.attr);
+#else
+	ret = class_create_file(uverbs_class, &class_attr_abi_version);
+#endif
 	if (ret) {
 		printk(KERN_ERR "user_verbs: couldn't create abi_version attribute\n");
 		goto out_class;
diff --git a/drivers/infiniband/hw/mlx4/main.c b/drivers/infiniband/hw/mlx4/main.c
index xxxxxxx..xxxxxxx xxxxxx
--- a/drivers/infiniband/hw/mlx4/main.c
+++ b/drivers/infiniband/hw/mlx4/main.c
@@ -1618,7 +1618,11 @@ static int del_gid_entry(struct ib_qp *ibqp, union ib_gid *gid)
 		rdma_get_mcast_mac((struct in6_addr *)gid, mac);
 		if (ndev) {
 			rtnl_lock();
+#ifdef HAVE_DEV_MC_DEL
 			dev_mc_del(ndev, mac);
+#else
+			dev_mc_delete(ndev, mac, 6, 0);
+#endif
 			rtnl_unlock();
 			dev_put(ndev);
 		}
@@ -1867,13 +1871,24 @@ static struct net_device *mlx4_ib_get_netdev(struct ib_device *device, u8 port_n
 {
 	struct mlx4_ib_dev *ibdev = to_mdev(device);
 
+#ifdef HAVE_BONDING_H
 	if (mlx4_is_bonded(ibdev->dev)) {
 		struct net_device *dev;
 		struct net_device *upper = NULL;
 
+		dev = mlx4_get_protocol_dev(ibdev->dev, MLX4_PROT_ETH, port_num);
+#ifdef HAVE_BOND_OPTION_ACTIVE_SLAVE_GET_SAFE
+
+		if (dev) {
+			rcu_read_lock();
+			upper = netdev_master_upper_dev_get_rcu(dev);
+			rcu_read_unlock();
+		}
+		if (upper)
+			dev = bond_option_active_slave_get_safe(netdev_priv(upper));
+#else
 		rcu_read_lock();
 
-		dev = mlx4_get_protocol_dev(ibdev->dev, MLX4_PROT_ETH, port_num);
 		if (dev)
 			upper = netdev_master_upper_dev_get_rcu(dev);
 		else
@@ -1882,9 +1897,10 @@ static struct net_device *mlx4_ib_get_netdev(struct ib_device *device, u8 port_n
 			dev = bond_option_active_slave_get_rcu(netdev_priv(upper));
 unlock:
 		rcu_read_unlock();
-
+#endif
 		return dev;
 	}
+#endif
 
 	return mlx4_get_protocol_dev(ibdev->dev, MLX4_PROT_ETH, port_num);
 }
@@ -2225,8 +2241,12 @@ static void mlx4_ib_alloc_eqs(struct mlx4_dev *dev, struct mlx4_ib_dev *ibdev)
 			sprintf(name, "mlx4-ib-%d-%d@pci:%s",
 				i, j, pci_name(dev->persist->pdev));
 			/* Set IRQ for specific name (per ring) */
+#ifdef HAVE_CPU_RMAP
 			if (mlx4_assign_eq(dev, name, NULL,
 					   &ibdev->eq_table[eq])) {
+#else
+			if (mlx4_assign_eq(dev, name, &ibdev->eq_table[eq])) {
+#endif
 				/* Use legacy (same as mlx4_en driver) */
 				pr_warn("Can't allocate EQ %d; reverting to legacy\n", eq);
 				ibdev->eq_table[eq] =
diff --git a/drivers/infiniband/hw/mlx4/mr.c b/drivers/infiniband/hw/mlx4/mr.c
index xxxxxxx..xxxxxxx xxxxxx
--- a/drivers/infiniband/hw/mlx4/mr.c
+++ b/drivers/infiniband/hw/mlx4/mr.c
@@ -76,8 +76,14 @@ static ssize_t shared_mr_proc_write(struct file *file,
 static int shared_mr_mmap(struct file *filep, struct vm_area_struct *vma)
 {
 
+#ifdef HAVE_PDE_DATA
 	struct mlx4_shared_mr_info *smr_info =
 		(struct mlx4_shared_mr_info *)PDE_DATA(filep->f_path.dentry->d_inode);
+#else
+	struct proc_dir_entry *pde = PDE(filep->f_path.dentry->d_inode);
+	struct mlx4_shared_mr_info *smr_info =
+		(struct mlx4_shared_mr_info *)pde->data;
+#endif
 
 	/* Prevent any mapping not on start of area */
 	if (vma->vm_pgoff != 0)
@@ -449,8 +455,10 @@ static int prepare_shared_mr(struct mlx4_ib_mr *mr, int access_flags, int mr_id)
 	struct proc_dir_entry *mr_proc_entry;
 	mode_t mode = S_IFREG;
 	char name_buff[128];
+#ifdef HAVE_PROC_SET_USER
 	kuid_t uid;
 	kgid_t gid;
+#endif
 
 	mode |= convert_shared_access(access_flags);
 	sprintf(name_buff, "%X", mr_id);
@@ -469,9 +477,14 @@ static int prepare_shared_mr(struct mlx4_ib_mr *mr, int access_flags, int mr_id)
 		return -ENODEV;
 	}
 
+#ifdef HAVE_PROC_SET_USER
 	current_uid_gid(&uid, &gid);
 	proc_set_user(mr_proc_entry, uid, gid);
 	proc_set_size(mr_proc_entry, mr->umem->length);
+#else
+	current_uid_gid(&(mr_proc_entry->uid), &(mr_proc_entry->gid));
+	mr_proc_entry->size = mr->umem->length;
+#endif
 
 	/* now creating an extra entry having a uniqe suffix counter */
 	mr->smr_info->counter = atomic64_inc_return(&shared_mr_count);
@@ -487,8 +500,13 @@ static int prepare_shared_mr(struct mlx4_ib_mr *mr, int access_flags, int mr_id)
 	}
 
 	mr->smr_info->counter_used = 1;
+#ifdef HAVE_PROC_SET_USER
 	proc_set_user(mr_proc_entry, uid, gid);
 	proc_set_size(mr_proc_entry, mr->umem->length);
+#else
+	current_uid_gid(&(mr_proc_entry->uid), &(mr_proc_entry->gid));
+	mr_proc_entry->size = mr->umem->length;
+#endif
 
 	return 0;
 
diff --git a/drivers/infiniband/hw/qib/qib_user_pages.c b/drivers/infiniband/hw/qib/qib_user_pages.c
index xxxxxxx..xxxxxxx xxxxxx
--- a/drivers/infiniband/hw/qib/qib_user_pages.c
+++ b/drivers/infiniband/hw/qib/qib_user_pages.c
@@ -73,8 +73,11 @@ static int __qib_get_user_pages(unsigned long start_page, size_t num_pages,
 		if (ret < 0)
 			goto bail_release;
 	}
-
+#ifdef HAVE_PINNED_VM
 	current->mm->pinned_vm += num_pages;
+#else
+	current->mm->locked_vm += num_pages;
+#endif
 
 	ret = 0;
 	goto bail;
@@ -151,7 +154,11 @@ void qib_release_user_pages(struct page **p, size_t num_pages)
 	__qib_release_user_pages(p, num_pages, 1);
 
 	if (current->mm) {
+#ifdef HAVE_PINNED_VM
 		current->mm->pinned_vm -= num_pages;
+#else
+		current->mm->locked_vm -= num_pages;
+#endif
 		up_write(&current->mm->mmap_sem);
 	}
 }
diff --git a/include/rdma/ib_pack.h b/include/rdma/ib_pack.h
index xxxxxxx..xxxxxxx xxxxxx
--- a/include/rdma/ib_pack.h
+++ b/include/rdma/ib_pack.h
@@ -34,7 +34,11 @@
 #define IB_PACK_H
 
 #include <rdma/ib_verbs.h>
+#ifdef HAVE_UAPI_LINUX_IF_ETHER_H
 #include <uapi/linux/if_ether.h>
+#else
+#include <linux/if_ether.h>
+#endif
 
 enum {
 	IB_LRH_BYTES  = 8,
diff --git a/include/rdma/ib_umem_odp.h b/include/rdma/ib_umem_odp.h
index xxxxxxx..xxxxxxx xxxxxx
--- a/include/rdma/ib_umem_odp.h
+++ b/include/rdma/ib_umem_odp.h
@@ -35,12 +35,14 @@
 
 #include <rdma/ib_umem.h>
 #include <rdma/ib_verbs.h>
+#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
 #include <linux/interval_tree.h>
 
 struct umem_odp_node {
 	u64 __subtree_last;
 	struct rb_node rb;
 };
+#endif
 
 struct ib_umem_odp {
 	/*
@@ -72,6 +74,7 @@ struct ib_umem_odp {
 	/* A linked list of umems that don't have private mmu notifier
 	 * counters yet. */
 	struct list_head no_private_counters;
+#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
 	struct ib_umem		*umem;
 
 	/* Tree tracking */
@@ -79,6 +82,7 @@ struct ib_umem_odp {
 
 	struct completion	notifier_completion;
 	int			dying;
+#endif
 };
 
 #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index xxxxxxx..xxxxxxx xxxxxx
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -39,6 +39,11 @@
 #if !defined(IB_VERBS_H)
 #define IB_VERBS_H
 
+#ifdef pr_fmt
+#undef pr_fmt
+#endif
+#define pr_fmt(fmt) fmt
+
 #include <linux/types.h>
 #include <linux/device.h>
 #include <linux/mm.h>
